2021-03-29
ⓚ금경용
<소스페이지에 가시면 형태소 소스 및 연관분석 소스를 볼 수 있습니다>

워드 임베딩(Word Embedding)
	워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 대표적인 방법으로
	주로 희소 표현에서 밀집 표현으로 변환하는 것을 의미

	★희소 표현(Sparse Representation)
 		- 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만
			1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법
		 - 벡터 또는 행렬(Matrix)의 값이 대부분이 0으로 표현되는 방법
 		- 원-핫 벡터는 희소 벡터(Sparse Vector)

	★밀집 표현(Dense Representation)
		 - 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
		 - 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됨
 		- 벡터의 차원이 조밀해졌다고 하여 밀집 벡터(Dense Vector)라고 함

	★워드 임베딩(Word Embedding)
 		- 단어를 밀집 벡터(Dense Vector)의 형태로 표현하는 방법
		- 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(Embedding 
			Vector)이라고 함


★Word2Vec
	단어 간 유사성을 고려하기 위해서는 단어의 의미를 벡터화하기 위한패키지

	희소 표현(Sparse Representation)
 		단어간 유사성을 표현할 수 없다는 단점
 	
	분산 표현(Distributed Representation)
 		단어의 '의미'를 다차원 공간에 벡터화하는 방법
		'비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'라는 가정
 		분산 표현을 이용하여 단어의 의미를 벡터화하는 작업은 워드 임베딩(Embedding) 작업에 속함

★class Word2Vec(sentences=None, size=100, window=5,min_count=5, workers=3)
	 size : 특징 벡터의 차원
 	window : 문장 내 현재 단어와 예측 단어 사이의 최대 거리
 	min_count : 총 빈도가 이보다 낮은 모든 단어를 무시함
 	workers : 모델을 훈련시킬 쓰레드의 수
 
	참고:https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/models/word2vec/index.html#models.word2vec.Word2Vec
 	패키지 설치 pip install gensim
