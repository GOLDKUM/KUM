2021-03-31
ⓚ금경용
<소스페이지에 가시면 형태소 소스 및 연관분석 소스를 볼 수 있습니다>
케라스를 이용한 인공신경망 구현 
	☆활성화 함수(activation function)
	-생물학적 뉴런(neuron)에서 입력 신호가 일정 크기 이상일 때만 신호를 전달하는 메커니즘을 모방한 함수
	-Softmax, Sigmoid, tanh(x), Binary step, Gaussian, ReLU

	☆인공신경망
	구성 요소 입력층(input layer), 출력층(output layer), 은닉층(hidden layer)
	 	
	■입력층
	□입력 값으로 구성된 레이어
 	□학습 데이터셋의 “입력 변수의 개수” 만큼의 노드로 구성
		□ 바이어스를 사용한다면 입력 변수의 수+1개 노드로 구성
	
	■출력층(output layer)
	□모델의 출력값을 만들어 내는 레이어
 	□예) IRIS 품종 분류 문제
		□멀티 클래스 분류 문제의 경우 소프트맥스(softmax) 함수를 출력함수로 사용
 
	■은닉층(hidden layer)
 	□입력층과 출력층 사이의 레이어
 	□뉴런(neuron)과 시냅스(synapse)로 구성된 인간의 두뇌를 모방하는 레이어
 	□은닉층으로 들어오는 입력값의 합을 계산한 후 활성화 함수를 적용
 	□활성화 함수 출력이 임계치를 넘지 않을 경우 다음 노드로 0을 전달(신호를 전달하지 않음)
 	□은닉층의 개수와 은닉 노드 개수
 	□너무 적으면 입력 데이터를 제대로 표현하지 못해 모델을 제대로 학습하지 못함
 	□너무 많으면 과적합(overfitting)이 발생하며, 학습 시간도 많이 소모
	
	DNN(Deep Neural Network)은 다층 인공신경망임
	
	■퍼셉트론
	□각 층의 뉴런들을 퍼셉트론(Perceptron) 퍼셉트론(Perceptron)은 인공 뉴런의 한 종류입니다.
	□TLU(threshold logic unit) 또는 LTU(linear threshold unit) 라 불리는 인공 뉴런 활용-하나의 층에 여러 개의 TLU로 구성됨
	□입력층의 뉴런의 수는 입력 데이터의 수이며, 출력층은 분류 문제를 해결할 경우에는 분류의 수와 일치

	■Keras
	□유저가 손쉽게 딥 러닝을 구현할 수 있도록 도와주는 상위 레벨의 인터페이스

















	

	☆