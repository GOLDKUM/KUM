{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:100% !important;}\n",
    "div.CodeMirror{font-family:Consolas; font-size:17pt;}\n",
    "div.output{font-size:17pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:17pt;}\n",
    "div.prompt{min-width:70px;}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd # 원핫인코딩\n",
    "from sklearn.model_selection import train_test_split # 훈련셋과 테스트셋 분리\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica\n",
       "0         1           0          0\n",
       "50        0           1          0\n",
       "100       0           0          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터 셋\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "# display(iris.head(2))\n",
    "iris_X = iris.iloc[:, :-1].to_numpy()\n",
    "iris_Y = iris.iloc[:, -1]\n",
    "# iris_Y = utils.to_categorical(iris_Y) # 숫자가 아니면 to_categorical 불가\n",
    "iris_Y = pd.get_dummies(iris_Y)\n",
    "iris_Y[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (105, 3), (45, 4), (45, 3))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "model_save_folder = './model/'\n",
    "if not os.path.exists(model_save_folder):\n",
    "    os.mkdir(model_save_folder)\n",
    "file = model_save_folder + 'iris-{epoch:03d}-val{val_accuracy:.4f}.h5'\n",
    "\n",
    "early_stopping = EarlyStopping(patience=40)\n",
    "checkpoint = ModelCheckpoint(filepath=file,\n",
    "                            monitor='val_accuracy',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True)\n",
    "\n",
    "# 1. 데이터 셋\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "# display(iris.head(2))\n",
    "iris_X = iris.iloc[:, :-1].to_numpy()\n",
    "iris_Y = iris.iloc[:, -1]\n",
    "# iris_Y = utils.to_categorical(iris_Y) # 숫자가 아니면 to_categorical 불가\n",
    "iris_Y = pd.get_dummies(iris_Y).to_numpy()\n",
    "# 1 0 0 = setosa  / 0 1 0 => versicolor / 0 0 1 => virginica\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(iris_X, iris_Y, test_size=0.3,\n",
    "                                                   random_state=1)\n",
    "train_X.shape, train_Y.shape, test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 182ms/step - loss: 1.1264 - accuracy: 0.2308 - val_loss: 0.9689 - val_accuracy: 0.2857\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.28571, saving model to ./model\\iris-001-val0.2857.h5\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9853 - accuracy: 0.3927 - val_loss: 0.9406 - val_accuracy: 0.3810\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.28571 to 0.38095, saving model to ./model\\iris-002-val0.3810.h5\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.9059 - accuracy: 0.5356 - val_loss: 0.9221 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.38095 to 0.52381, saving model to ./model\\iris-003-val0.5238.h5\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8441 - accuracy: 0.7454 - val_loss: 0.9095 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.52381\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7995 - accuracy: 0.7521 - val_loss: 0.8865 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.52381\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7688 - accuracy: 0.7254 - val_loss: 0.8631 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.52381\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7200 - accuracy: 0.7454 - val_loss: 0.8407 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.52381\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6952 - accuracy: 0.7321 - val_loss: 0.8144 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.52381\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6627 - accuracy: 0.7387 - val_loss: 0.7889 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.52381\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6230 - accuracy: 0.7454 - val_loss: 0.7613 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.52381\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5982 - accuracy: 0.7387 - val_loss: 0.7309 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.52381\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5924 - accuracy: 0.7054 - val_loss: 0.7003 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.52381\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5466 - accuracy: 0.7321 - val_loss: 0.6782 - val_accuracy: 0.5238\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.52381\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5128 - accuracy: 0.7454 - val_loss: 0.6550 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.52381 to 0.57143, saving model to ./model\\iris-014-val0.5714.h5\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4977 - accuracy: 0.7387 - val_loss: 0.6296 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.57143\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4654 - accuracy: 0.7454 - val_loss: 0.6022 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.57143 to 0.66667, saving model to ./model\\iris-016-val0.6667.h5\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4549 - accuracy: 0.7851 - val_loss: 0.5752 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.66667 to 0.71429, saving model to ./model\\iris-017-val0.7143.h5\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4181 - accuracy: 0.8489 - val_loss: 0.5550 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.71429\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4088 - accuracy: 0.8714 - val_loss: 0.5350 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.71429\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3855 - accuracy: 0.8860 - val_loss: 0.5170 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.71429 to 0.76190, saving model to ./model\\iris-020-val0.7619.h5\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3934 - accuracy: 0.8740 - val_loss: 0.4971 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.76190 to 0.85714, saving model to ./model\\iris-021-val0.8571.h5\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3795 - accuracy: 0.9257 - val_loss: 0.4851 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.85714\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3446 - accuracy: 0.9324 - val_loss: 0.4746 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.85714\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3369 - accuracy: 0.9324 - val_loss: 0.4648 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.85714\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3329 - accuracy: 0.9390 - val_loss: 0.4494 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.85714\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3164 - accuracy: 0.9324 - val_loss: 0.4332 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.85714\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3182 - accuracy: 0.9470 - val_loss: 0.4179 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.85714\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2976 - accuracy: 0.9549 - val_loss: 0.4020 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.85714\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3027 - accuracy: 0.9695 - val_loss: 0.3946 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.85714\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2841 - accuracy: 0.9629 - val_loss: 0.3854 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.85714\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2748 - accuracy: 0.9629 - val_loss: 0.3754 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.85714\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2792 - accuracy: 0.9629 - val_loss: 0.3688 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.85714\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2402 - accuracy: 0.9762 - val_loss: 0.3646 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.85714\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2467 - accuracy: 0.9629 - val_loss: 0.3504 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.85714\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2510 - accuracy: 0.9562 - val_loss: 0.3337 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.85714 to 0.95238, saving model to ./model\\iris-035-val0.9524.h5\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2525 - accuracy: 0.9708 - val_loss: 0.3250 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.95238\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2344 - accuracy: 0.9775 - val_loss: 0.3234 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.95238\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2204 - accuracy: 0.9629 - val_loss: 0.3172 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.95238\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2200 - accuracy: 0.9629 - val_loss: 0.3075 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.95238\n",
      "Epoch 40/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2081 - accuracy: 0.9775 - val_loss: 0.2951 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.95238\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2065 - accuracy: 0.9708 - val_loss: 0.2863 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.95238\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1964 - accuracy: 0.9708 - val_loss: 0.2765 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.95238\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1882 - accuracy: 0.9708 - val_loss: 0.2748 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.95238\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1983 - accuracy: 0.9708 - val_loss: 0.2678 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.95238\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1704 - accuracy: 0.9775 - val_loss: 0.2687 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.95238\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1829 - accuracy: 0.9775 - val_loss: 0.2529 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.95238\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1757 - accuracy: 0.9708 - val_loss: 0.2399 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.95238\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1704 - accuracy: 0.9775 - val_loss: 0.2330 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.95238\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1627 - accuracy: 0.9708 - val_loss: 0.2314 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.95238\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1641 - accuracy: 0.9775 - val_loss: 0.2331 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.95238\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1561 - accuracy: 0.9708 - val_loss: 0.2248 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.95238\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1458 - accuracy: 0.9775 - val_loss: 0.2176 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.95238\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1415 - accuracy: 0.9708 - val_loss: 0.2080 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.95238\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1254 - accuracy: 0.9841 - val_loss: 0.2047 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.95238\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1442 - accuracy: 0.9708 - val_loss: 0.1994 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.95238\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1433 - accuracy: 0.9708 - val_loss: 0.2012 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.95238\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1275 - accuracy: 0.9775 - val_loss: 0.1986 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.95238\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1364 - accuracy: 0.9708 - val_loss: 0.1903 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.95238\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1367 - accuracy: 0.9708 - val_loss: 0.1874 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.95238\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1320 - accuracy: 0.9708 - val_loss: 0.1816 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.95238\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1097 - accuracy: 0.9841 - val_loss: 0.1794 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.95238\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1249 - accuracy: 0.9708 - val_loss: 0.1750 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.95238\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1220 - accuracy: 0.9708 - val_loss: 0.1738 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.95238\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1183 - accuracy: 0.9708 - val_loss: 0.1747 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.95238\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1110 - accuracy: 0.9775 - val_loss: 0.1759 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.95238\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0926 - accuracy: 0.9841 - val_loss: 0.1689 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.95238\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1092 - accuracy: 0.9775 - val_loss: 0.1624 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00067: val_accuracy improved from 0.95238 to 1.00000, saving model to ./model\\iris-067-val1.0000.h5\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1171 - accuracy: 0.9708 - val_loss: 0.1605 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 1.00000\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0989 - accuracy: 0.9841 - val_loss: 0.1607 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 1.00000\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1024 - accuracy: 0.9775 - val_loss: 0.1612 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 1.00000\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0973 - accuracy: 0.9775 - val_loss: 0.1572 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 1.00000\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1069 - accuracy: 0.9708 - val_loss: 0.1545 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 1.00000\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1031 - accuracy: 0.9708 - val_loss: 0.1523 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 1.00000\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0840 - accuracy: 0.9775 - val_loss: 0.1515 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 1.00000\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0870 - accuracy: 0.9841 - val_loss: 0.1520 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 1.00000\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1000 - accuracy: 0.9775 - val_loss: 0.1485 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 1.00000\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1001 - accuracy: 0.9708 - val_loss: 0.1474 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 1.00000\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0943 - accuracy: 0.9775 - val_loss: 0.1455 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 1.00000\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0889 - accuracy: 0.9775 - val_loss: 0.1448 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 1.00000\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0820 - accuracy: 0.9841 - val_loss: 0.1455 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 1.00000\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0794 - accuracy: 0.9841 - val_loss: 0.1424 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 1.00000\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0878 - accuracy: 0.9775 - val_loss: 0.1438 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 1.00000\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0954 - accuracy: 0.9775 - val_loss: 0.1412 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 1.00000\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0899 - accuracy: 0.9775 - val_loss: 0.1404 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 1.00000\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0753 - accuracy: 0.9841 - val_loss: 0.1400 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 1.00000\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0801 - accuracy: 0.9775 - val_loss: 0.1385 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 1.00000\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0952 - accuracy: 0.9708 - val_loss: 0.1381 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 1.00000\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0765 - accuracy: 0.9775 - val_loss: 0.1373 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 1.00000\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0910 - accuracy: 0.9708 - val_loss: 0.1386 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 1.00000\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0784 - accuracy: 0.9775 - val_loss: 0.1407 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 1.00000\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0915 - accuracy: 0.9708 - val_loss: 0.1356 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 1.00000\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0721 - accuracy: 0.9841 - val_loss: 0.1347 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 1.00000\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0667 - accuracy: 0.9841 - val_loss: 0.1355 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 1.00000\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0776 - accuracy: 0.9775 - val_loss: 0.1355 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 1.00000\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0906 - accuracy: 0.9708 - val_loss: 0.1338 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 1.00000\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0870 - accuracy: 0.9708 - val_loss: 0.1333 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 1.00000\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0843 - accuracy: 0.9775 - val_loss: 0.1366 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 1.00000\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0672 - accuracy: 0.9841 - val_loss: 0.1337 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 1.00000\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0737 - accuracy: 0.9775 - val_loss: 0.1332 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 1.00000\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0723 - accuracy: 0.9775 - val_loss: 0.1375 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 1.00000\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0864 - accuracy: 0.9775 - val_loss: 0.1316 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 1.00000\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0761 - accuracy: 0.9775 - val_loss: 0.1301 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 1.00000\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0858 - accuracy: 0.9708 - val_loss: 0.1298 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 1.00000\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0715 - accuracy: 0.9775 - val_loss: 0.1297 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 1.00000\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0874 - accuracy: 0.9708 - val_loss: 0.1302 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 1.00000\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0777 - accuracy: 0.9775 - val_loss: 0.1297 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 1.00000\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0707 - accuracy: 0.9775 - val_loss: 0.1295 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 1.00000\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0872 - accuracy: 0.9708 - val_loss: 0.1294 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 1.00000\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0657 - accuracy: 0.9775 - val_loss: 0.1296 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 1.00000\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0835 - accuracy: 0.9708 - val_loss: 0.1290 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 1.00000\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0659 - accuracy: 0.9841 - val_loss: 0.1288 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 1.00000\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0810 - accuracy: 0.9708 - val_loss: 0.1296 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 1.00000\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0797 - accuracy: 0.9708 - val_loss: 0.1289 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 1.00000\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0805 - accuracy: 0.9708 - val_loss: 0.1282 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 1.00000\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0831 - accuracy: 0.9708 - val_loss: 0.1288 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 1.00000\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0728 - accuracy: 0.9775 - val_loss: 0.1287 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 1.00000\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0584 - accuracy: 0.9841 - val_loss: 0.1277 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 1.00000\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0740 - accuracy: 0.9775 - val_loss: 0.1332 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 1.00000\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0750 - accuracy: 0.9775 - val_loss: 0.1394 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 1.00000\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0784 - accuracy: 0.9708 - val_loss: 0.1307 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 1.00000\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0796 - accuracy: 0.9708 - val_loss: 0.1300 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 1.00000\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0724 - accuracy: 0.9775 - val_loss: 0.1365 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00122: val_accuracy did not improve from 1.00000\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0870 - accuracy: 0.9708 - val_loss: 0.1281 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 1.00000\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0636 - accuracy: 0.9775 - val_loss: 0.1284 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 1.00000\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0660 - accuracy: 0.9775 - val_loss: 0.1333 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 1.00000\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0693 - accuracy: 0.9775 - val_loss: 0.1326 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 1.00000\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0785 - accuracy: 0.9775 - val_loss: 0.1290 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 1.00000\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0762 - accuracy: 0.9708 - val_loss: 0.1281 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 1.00000\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0612 - accuracy: 0.9775 - val_loss: 0.1266 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 1.00000\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0774 - accuracy: 0.9708 - val_loss: 0.1266 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 1.00000\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0596 - accuracy: 0.9841 - val_loss: 0.1269 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 1.00000\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0730 - accuracy: 0.9775 - val_loss: 0.1281 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 1.00000\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0783 - accuracy: 0.9708 - val_loss: 0.1395 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 1.00000\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0653 - accuracy: 0.9841 - val_loss: 0.1343 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 1.00000\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0559 - accuracy: 0.9841 - val_loss: 0.1311 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 1.00000\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0620 - accuracy: 0.9775 - val_loss: 0.1289 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 1.00000\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0777 - accuracy: 0.9708 - val_loss: 0.1273 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 1.00000\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0637 - accuracy: 0.9775 - val_loss: 0.1267 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 1.00000\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0709 - accuracy: 0.9775 - val_loss: 0.1277 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 1.00000\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0770 - accuracy: 0.9708 - val_loss: 0.1330 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 1.00000\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0643 - accuracy: 0.9775 - val_loss: 0.1318 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 1.00000\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0745 - accuracy: 0.9708 - val_loss: 0.1313 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 1.00000\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0566 - accuracy: 0.9841 - val_loss: 0.1272 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 1.00000\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0590 - accuracy: 0.9775 - val_loss: 0.1286 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 1.00000\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0555 - accuracy: 0.9841 - val_loss: 0.1313 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 1.00000\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0740 - accuracy: 0.9708 - val_loss: 0.1414 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 1.00000\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.1305 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 1.00000\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0696 - accuracy: 0.9708 - val_loss: 0.1280 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 1.00000\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0697 - accuracy: 0.9775 - val_loss: 0.1289 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 1.00000\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0659 - accuracy: 0.9775 - val_loss: 0.1276 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 1.00000\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0579 - accuracy: 0.9775 - val_loss: 0.1295 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 1.00000\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0514 - accuracy: 0.9841 - val_loss: 0.1370 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 1.00000\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0653 - accuracy: 0.9854 - val_loss: 0.1436 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 1.00000\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0679 - accuracy: 0.9775 - val_loss: 0.1314 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 1.00000\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0738 - accuracy: 0.9708 - val_loss: 0.1281 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 1.00000\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0766 - accuracy: 0.9708 - val_loss: 0.1287 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 1.00000\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0565 - accuracy: 0.9775 - val_loss: 0.1282 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0511 - accuracy: 0.9841 - val_loss: 0.1307 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0576 - accuracy: 0.9775 - val_loss: 0.1476 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0716 - accuracy: 0.9854 - val_loss: 0.1491 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0559 - accuracy: 0.9921 - val_loss: 0.1292 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0738 - accuracy: 0.9708 - val_loss: 0.1325 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0606 - accuracy: 0.9775 - val_loss: 0.1323 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0774 - accuracy: 0.9708 - val_loss: 0.1307 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0724 - accuracy: 0.9708 - val_loss: 0.1393 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0619 - accuracy: 0.9775 - val_loss: 0.1387 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0602 - accuracy: 0.9775 - val_loss: 0.1310 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0660 - accuracy: 0.9775 - val_loss: 0.1305 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0642 - accuracy: 0.9775 - val_loss: 0.1339 - val_accuracy: 0.9524\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0708 - accuracy: 0.9708 - val_loss: 0.1389 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19817f93520>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=4, activation=\"relu\"))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# 3. 학습과정 설정\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# 4. 학습\n",
    "hist = model.fit(train_X, train_Y, batch_size=50, epochs=300,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "# 5.학습과정 표시하기\n",
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(hist.history['loss'], 'y', label=\"train loss\")\n",
    "loss_ax.plot(hist.history['val_loss'], 'g', label=\"val loss\")\n",
    "loss_ax.set_xlabel(\"epochs\")\n",
    "loss_ax.set_ylabel(\"loss\")\n",
    "\n",
    "acc_ax = loss_ax.twinx() # x축 공유하는 acc_ax\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label=\"train accuracy\")\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'r', label=\"val accuracy\")\n",
    "acc_ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "loss_ax.legend(loc=\"upper left\")\n",
    "acc_ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2,\n",
       "       1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "real = np.argmax(test_Y, axis=1) # 실제값\n",
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2,\n",
       "       1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict_classes(test_X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(pred==real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred   0   1   2\n",
       "real            \n",
       "0     14   0   0\n",
       "1      0  18   0\n",
       "2      0   0  13"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_result = pd.crosstab(real, pred)\n",
    "ct_result.index.name = \"real\"\n",
    "ct_result.columns.name = \"pred\"\n",
    "ct_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 998us/step - loss: 0.0344 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "score = model.evaluate(test_X, test_Y, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가된 loss : 0.034357305616140366\n",
      "평가된 accuracy : 1.0\n",
      "[0.034357305616140366, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"평가된 loss :\", score[0])\n",
    "print(\"평가된 accuracy :\", score[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length           6.4\n",
       "sepal_width            3.2\n",
       "petal_length           4.5\n",
       "petal_width            1.5\n",
       "species         versicolor\n",
       "Name: 51, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.iloc[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(np.array([[6.4, 3.2,4.5,1.5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
